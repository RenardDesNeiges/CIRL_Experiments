{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization Notebook\n",
    "\n",
    "This notebook is dedicated to make my gradient evaluation code fast by using the XLA (Accelerated Linear Algebra) vector operations from jax to compute them (instead of the insanely slow python iterators I've previously been using). This is a bit of a pain to write though, hence the dedicated notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Includes \"\"\"\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import jax.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from env.mdp import Sampler, MarkovDecisionProcess\n",
    "from env.gridworld import Gridworld, gridplot\n",
    "from algs.policy_gradients import PolicyGradientMethod, vanillaGradOracle, naturalGradOracle, monteCarloVanillaGrad, Sampler, monteCarloNaturalGrad\n",
    "from itertools import accumulate\n",
    "\n",
    "def flatten(v):\n",
    "    return jnp.reshape(v,(list(accumulate(v.shape,lambda x,y:x*y))[-1],))\n",
    "\n",
    "\"\"\" Initialize the Jax RNG \"\"\"\n",
    "key = jax.random.PRNGKey(0) \n",
    "from jax.config import config; config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating some example MDP\n",
    "We instantiate a $2Â \\times 1$ `gridworld`, to have the simplest possible example for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x160 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "R = 100\n",
    "goals = [((1,0),R)]\n",
    "gridMDP = Gridworld(2,1,0.1,0.9,goals=goals,obstacles=[]) \n",
    "gridMDP.init_distrib =  jnp.exp(jax.random.uniform(key,(gridMDP.n,))) / \\\n",
    "    jnp.sum(jnp.exp(jax.random.uniform(key,(gridMDP.n,))))\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(1,figsize=(2,1.6))\n",
    "gridplot(gridMDP,ax,goals=goals)\n",
    "ax.set_title('Gridworld structure')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also choose to use a direct `softmax` policy parametrization, which we define here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We define our policy parametrization\"\"\"\n",
    "parametrization = lambda p : nn.softmax(p,axis=1)\n",
    "\"\"\"As well as some random policy parameter vector\"\"\"\n",
    "theta = jax.random.uniform(key,(gridMDP.n,gridMDP.m)) # where the dimensionality matches the MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling an MDP\n",
    "\n",
    "A quick profiling of the naive (very very slow) implementation of GPOMDP tells us that about $\\approx 90\\%$ of the performance is lost in the GPOMDP gradient computation (and not in the batch sampling), so although on paper it should be possible to paralellize pretty well for discrete MDPs - when sampling batches - we decide not to consider vectorization of the sampling routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch object is an object of class <class 'list'> (of length 5, which is the batch size), \n",
      " which contains trajectories (which are of <class 'list'> and of lenght 3, which is the horizon). \n",
      " Each trajectory element is a <class 'tuple'> which contains 3 elements: the state, the action and the reward.\n",
      "\n",
      "The batch that we just sampled prints as: \n",
      "\n",
      "[[(DeviceArray(0, dtype=int64), DeviceArray(1, dtype=int64), 0), (DeviceArray(0, dtype=int64), DeviceArray(1, dtype=int64), -1.0), (DeviceArray(0, dtype=int64), DeviceArray(1, dtype=int64), -1.0)], [(DeviceArray(1, dtype=int64), DeviceArray(3, dtype=int64), 0), (DeviceArray(1, dtype=int64), DeviceArray(3, dtype=int64), -1.0), (DeviceArray(1, dtype=int64), DeviceArray(3, dtype=int64), -1.0)], [(DeviceArray(0, dtype=int64), DeviceArray(1, dtype=int64), 0), (DeviceArray(0, dtype=int64), DeviceArray(1, dtype=int64), -1.0), (DeviceArray(0, dtype=int64), DeviceArray(1, dtype=int64), -1.0)], [(DeviceArray(0, dtype=int64), DeviceArray(1, dtype=int64), 0), (DeviceArray(0, dtype=int64), DeviceArray(1, dtype=int64), -1.0), (DeviceArray(0, dtype=int64), DeviceArray(1, dtype=int64), -1.0)], [(DeviceArray(1, dtype=int64), DeviceArray(3, dtype=int64), 0), (DeviceArray(1, dtype=int64), DeviceArray(3, dtype=int64), -1.0), (DeviceArray(1, dtype=int64), DeviceArray(3, dtype=int64), -1.0)]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" We must define an horizon and a batch size \"\"\"\n",
    "HORIZON = 3\n",
    "BATCH = 5\n",
    "\n",
    "\"\"\" Generic trajectory sampling routine for MDPs \"\"\"\n",
    "def sample_trajectory(pi : jnp.ndarray ,mdp: MarkovDecisionProcess,smp : Sampler,H : int,key : jax.random.PRNGKeyArray):\n",
    "    \"\"\" Samples a single trajectory\n",
    "    \"\"\"\n",
    "    def pick_action(pi : jnp.ndarray,s:int,mdp:MarkovDecisionProcess):\n",
    "        p = pi[s]; p /= jnp.sum(p)\n",
    "        return jax.random.choice(key,jnp.arange(mdp.m), p = p)\n",
    "        \n",
    "    traj = []\n",
    "    r_t = 0\n",
    "    s_t = smp.reset(key=key)\n",
    "    for _ in range(H):\n",
    "        a_t = pick_action(pi,s_t,mdp)\n",
    "        traj += [(s_t,a_t,r_t)]\n",
    "        s_t, r_t = smp.step(a_t)\n",
    "    return traj\n",
    "\n",
    "\"\"\" Samples a batch of trajectories \"\"\"\n",
    "def sample_batch(pi:jnp.ndarray,mdp:MarkovDecisionProcess,smp:Sampler,H:int,B:int,key:jax.random.PRNGKeyArray):\n",
    "    \"\"\" Samples a batch of trajectories\n",
    "    \"\"\"\n",
    "    subkeys = jax.random.split(key,B)\n",
    "    return [sample_trajectory(pi,mdp,smp,H,k) for k in subkeys]\n",
    "\n",
    "\"\"\" The sampler object handlers random sampling of MDP steps \"\"\"\n",
    "sampler = Sampler(gridMDP,key)\n",
    "\n",
    "batch = sample_batch(parametrization(theta),gridMDP,sampler,HORIZON,BATCH,key)\n",
    "\n",
    "print(f'The batch object is an object of class {type(batch)} (of length {len(batch)}, which is the batch size), \\n which contains trajectories (which are of {type(batch[0])} and of lenght {len(batch[0])}, which is the horizon). \\n Each trajectory element is a {type(batch[0][0])} which contains 3 elements: the state, the action and the reward.')\n",
    "print('\\nThe batch that we just sampled prints as: \\n')\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing GPOMDP gradients the ugly (and really slow) way\n",
    "\n",
    "The `GPOMDP` estimator which has the following form:\n",
    "$$\n",
    "g_\\theta = \n",
    "\\frac{1}{B} \\sum^B_{i=1} \\sum^{H-1}_{h=0} r_h^i \\gamma^h\n",
    "\\Big( \\sum^{h}_{j=0} \\nabla_\\theta \\log \\pi_\\theta(a_j^i|s_j^i)  \\Big)\n",
    "\\approx \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "Which lends itself very easily to an incredibly slow python iterator implementation. Which we give below. This implementation is tested and we know that it gives correct gradients, so we will use it as a way of checking the validity of our vectorized `jax` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpomdp(batch,theta,B,H,gamma,parametrization):\n",
    "    def g_log(theta,s,a):\n",
    "        return jax.grad(lambda p : jnp.log(parametrization(p))[s,a])(theta)\n",
    "    def trace_grad(batch,theta,b,h):\n",
    "        return jnp.sum(jnp.array([g_log(theta,e[0],e[1]) for e in batch[b][:h]]),axis=0)\n",
    "    def single_sample_gpomdp(batch,theta,b,H):\n",
    "        return jnp.sum(jnp.array([(gamma**h)*batch[b][h][2] \\\n",
    "                    *trace_grad(batch,theta,b,h) for h in range(1,H)]),axis=0) #something is fucked here actually\n",
    "        \n",
    "    return (1/B)*jnp.sum(jnp.array([single_sample_gpomdp(batch,theta,b,H) for b in range(B)]),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a gradient of shape $S \\times A$, which here is $2 \\times 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.30037406 -1.04648386  0.32618588  0.41992391]\n",
      " [ 0.25095342  0.16381175  0.2803066  -0.69507177]] (2, 4)\n"
     ]
    }
   ],
   "source": [
    "reference_gradient = gpomdp(batch,theta,BATCH,HORIZON,gridMDP.gamma,parametrization)\n",
    "print(reference_gradient, reference_gradient.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized implementation of the gradient estimator\n",
    "The general idea behind the vectorized implementation is to \n",
    "\n",
    "0) properly batch the data\n",
    "1) compute all the gradients $\\nabla_\\theta \\log \\pi_\\theta(a_j^i|s_j^i)$ in parallel (using a `jax.vmap` operator)\n",
    "2) compute the sums $\\sum^{h}_{j=0} \\nabla_\\theta \\log \\pi_\\theta(a_j^i|s_j^i)$ using a parallelized cumulative sum operator\n",
    "3) compute the terms $r_h^i \\gamma^h \\Big( \\sum^{h}_{j=0} \\nabla_\\theta \\log \\pi_\\theta(a_j^i|s_j^i)  \\Big)$ in parrallel using elementwise mutliplication\n",
    "4) sum everything together along the batch and horizon axes to get the gradient estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have two integer arrays (for state and actions) and a float array (for rewards)\n",
      "All arrays have the same shape:\n",
      "(5, 3) (5, 3) (5, 3)\n",
      ",which is (BATCH, HORIZON).\n"
     ]
    }
   ],
   "source": [
    "\"\"\" First (0.) we batch the data together into three separate jax deviceArrays\"\"\"\n",
    "s_batch = jnp.array([[e[0] for e in s] for s in batch])\n",
    "a_batch = jnp.array([[e[1] for e in s] for s in batch])\n",
    "r_batch = jnp.array([[e[2] for e in s] for s in batch])\n",
    "print(\"We have two integer arrays (for state and actions) and a float array (for rewards)\")\n",
    "print(\"All arrays have the same shape:\")\n",
    "print(s_batch.shape,a_batch.shape, r_batch.shape)\n",
    "print(\",which is (BATCH, HORIZON).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This generates quite a complex shaped tensor, it has shape: (5, 3, 2, 4), \n",
      "which is (BATCH,HORIZON,STATE,ACTION), if we slice the array \n",
      "and look at a single gradient in the gradient tensor what we have is something like that:\n",
      "[[-0.1986601   0.69211895 -0.2157314  -0.27772746]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "Which is the gradient of the log of the policy parametrization of whatever action was taken at \n",
      "some state action pair (s,a) corresponding to some step of some trajectory in the batch.\n",
      "This element is of shape (2, 4), whcih stands for (STATE,ACTION).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Next (1.) we compute the batched gradients\"\"\"\n",
    "\n",
    "\"\"\"This operation has multiple steps, first we define a gradient of log function and use \n",
    "a lambda expression to get it to have a prototype which is easier to vmap over later\"\"\"\n",
    "def g_log(s,a,theta,parametrization):\n",
    "    return jax.grad(lambda p : jnp.log(parametrization(p))[s,a])(theta)\n",
    "_f = lambda s,a : g_log(s,a,theta,parametrization)\n",
    "\n",
    "\"\"\"The we generate ou gradient array by using a double jax.vmap operation (which is required because\n",
    "    we are vmapping over two different axes)\"\"\"\n",
    "batch_grads = jax.vmap(jax.vmap(_f))(s_batch, a_batch) ##vmap can only operate on a single axis \n",
    "\n",
    "print(f\"This generates quite a complex shaped tensor, it has shape: {batch_grads.shape}, \\nwhich is (BATCH,HORIZON,STATE,ACTION), if we slice the array \\nand look at a single gradient in the gradient tensor what we have is something like that:\")\n",
    "print(batch_grads[0,0,:,:])\n",
    "print(f\"Which is the gradient of the log of the policy parametrization of whatever action was taken at \\nsome state action pair (s,a) corresponding to some step of some trajectory in the batch.\\nThis element is of shape {batch_grads[0,0,:,:].shape}, whcih stands for (STATE,ACTION).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for (a0, s0) at all time steps in trajectory 0 of the batch:        \n",
      "[(0, -0.1986600953106976), (1, -0.1986600953106976), (2, -0.1986600953106976)]        \n",
      "Accumulated gradients for (a0, s0) at all time steps in trajectory 0 of the batch:        \n",
      "[(0, -0.1986600953106976), (1, -0.3973201906213952), (2, -0.5959802859320927)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"What we do next (2.) is that we take a cumulative sum of the batched gradients along the horizon axis\"\"\"\n",
    "\n",
    "# Recall the grad tensor is of shape (BATCH,HORIZON,STATE,ACTION), so axis 1 is the time axis\n",
    "summed_grads = jnp.cumsum(batch_grads,axis=1) \n",
    "\n",
    "print(f\"Gradients for (a0, s0) at all time steps in trajectory 0 of the batch:\\\n",
    "        \\n{[(t,float(batch_grads[0,t,0,0])) for t in range(HORIZON)]}\\\n",
    "        \\nAccumulated gradients for (a0, s0) at all time steps in trajectory 0 of the batch:\\\n",
    "        \\n{[(t,float(summed_grads[0,t,0,0])) for t in range(HORIZON)]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 2, 4)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which gives us a tensor of the same shape as the previous one: (5, 3, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Then (3.) we multiply our accumulated gradients with the discount factors and rewards\"\"\"\n",
    "\n",
    "gamma_tensor = gridMDP.gamma**jnp.arange(HORIZON) #here we build a discount factor tensor of the right shape\n",
    "gamma_tensor = jnp.repeat(jnp.repeat(jnp.repeat(\n",
    "    gamma_tensor[jnp.newaxis, :, jnp.newaxis, jnp.newaxis], \n",
    "        BATCH, axis=0), summed_grads.shape[2],axis=2), summed_grads.shape[3],axis=3)\n",
    "reward_tensor = jnp.repeat(jnp.repeat(\n",
    "    r_batch[:, :, jnp.newaxis, jnp.newaxis], \n",
    "    summed_grads.shape[2], axis=2),\n",
    "        summed_grads.shape[3],axis=3) #here we repeat the reward along the right axes so we can elementwise multiply with the gradients\n",
    "\n",
    "gradient_tensor = summed_grads * reward_tensor * gamma_tensor # finally we get our gradients\n",
    "print(f'Which gives us a tensor of the same shape as the previous one: {gradient_tensor.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final gradient is thus:\n",
      "[[ 0.50419932 -1.7565979   0.54752629  0.70487229]\n",
      " [ 0.42124325  0.27496972  0.47051464 -1.16672761]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Finally (4.) summing over the batch and horizon axes we get our gradient value\"\"\"\n",
    "fast_gradient = (1/BATCH)*jnp.sum(gradient_tensor,axis=(0,1))\n",
    "print(f\"The final gradient is thus:\\n{fast_gradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.30037406, -1.04648386,  0.32618588,  0.41992391],\n",
       "             [ 0.25095342,  0.16381175,  0.2803066 , -0.69507177]],            dtype=float64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1.  , 0.9 , 0.81], dtype=float64, weak_type=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_tensor[0,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLExperiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
